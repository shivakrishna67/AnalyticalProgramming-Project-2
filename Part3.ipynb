{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfbHpMWf0U-j",
    "outputId": "85733dbb-f4ad-465e-a14e-6ea2e2410904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter num of articles: 10\n",
      "Fetched 10 articles\n",
      "Number of records: 10\n",
      "Dimensions of articles: (10, 6)\n",
      "                                               Title  \\\n",
      "0   Information retrieval as statistical translation   \n",
      "1  A survey of automatic query expansion in infor...   \n",
      "2  A language modeling approach to information re...   \n",
      "3  A study of smoothing methods for language mode...   \n",
      "4  Integrating and evaluating neural word embeddi...   \n",
      "\n",
      "                          Author  Year   Published  \\\n",
      "0           A Berger, J Lafferty  2017  dl.acm.org   \n",
      "1          C Carpineto, G Romano  2012  dl.acm.org   \n",
      "2             JM Ponte, WB Croft  2017  dl.acm.org   \n",
      "3             C Zhai, J Lafferty  2017  dl.acm.org   \n",
      "4  G Zuccon, B Koopman, P Bruza…    20  dl.acm.org   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  … There is a large literature on probabilistic...   \n",
      "1  … information retrieval systems is largely cau...   \n",
      "2  … models, we have developed an approach to ret...   \n",
      "3  … to information retrieval are attractive and ...   \n",
      "4  … in information retrieval. Specifically, we f...   \n",
      "\n",
      "                                        Abstract_UrL  \n",
      "0  https://dl.acm.org/doi/abs/10.1145/3130348.313...  \n",
      "1  https://dl.acm.org/doi/abs/10.1145/2071389.207...  \n",
      "2  https://dl.acm.org/doi/pdf/10.1145/3130348.313...  \n",
      "3  https://dl.acm.org/doi/abs/10.1145/3130348.313...  \n",
      "4  https://dl.acm.org/doi/abs/10.1145/2838931.283...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Scholarly:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Scholarly class.\n",
    "\n",
    "        This class provides methods to fetch and parse information from Google Scholar articles.\n",
    "\n",
    "        Parameters:\n",
    "        - None\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"\n",
    "        Retrieve and parse HTML content from a given URL.\n",
    "\n",
    "        Parameters:\n",
    "        - url (str): The URL of the page to fetch.\n",
    "\n",
    "        Returns:\n",
    "        - BeautifulSoup: The parsed HTML content.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\"\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(url, headers)\n",
    "            if data.status_code != 200:\n",
    "                raise Exception(\"Failed to fetch data\")\n",
    "        except Exception as ex:\n",
    "            print(\n",
    "                f\"Exception occurred: {data.text} with status_code {data.status_code}\"\n",
    "            )\n",
    "            return None\n",
    "        soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    def get_title(self, title):\n",
    "        \"\"\"\n",
    "        Extract the title of an article from the provided BeautifulSoup object.\n",
    "\n",
    "        Parameters:\n",
    "        - title (BeautifulSoup): The BeautifulSoup object representing the title of an article.\n",
    "\n",
    "        Returns:\n",
    "        - str: The text title of the article.\n",
    "        \"\"\"\n",
    "        return str(title.find(\"a\").text)\n",
    "\n",
    "    def get_abstract_url(self, title):\n",
    "        \"\"\"\n",
    "        Extract the URL for the title of an article from the provided BeautifulSoup object.\n",
    "\n",
    "        Parameters:\n",
    "        - title (BeautifulSoup): The BeautifulSoup object representing the title of an article.\n",
    "\n",
    "        Returns:\n",
    "        - str: The URL for the title of the article.\n",
    "        \"\"\"\n",
    "        return str(title.find(\"a\").get(\"href\"))\n",
    "\n",
    "    def get_article_info(self, article):\n",
    "        \"\"\"\n",
    "        Extract author, year, and published information from the provided BeautifulSoup object.\n",
    "\n",
    "        Parameters:\n",
    "        - article (BeautifulSoup): The BeautifulSoup object representing an article.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing author, year, and published information.\n",
    "        \"\"\"\n",
    "        year = int(re.search(r\"\\d+\", article.text).group())\n",
    "        article = str(article.text).replace(\"\\xa0\", \"\")\n",
    "        article = article.split(\"-\")\n",
    "        published = article[-1].strip()\n",
    "        author = article[0].strip()\n",
    "        return author, year, published\n",
    "\n",
    "    def get_tags(self, soup):\n",
    "        \"\"\"\n",
    "        Extract information from various tags in the provided BeautifulSoup object.\n",
    "\n",
    "        Parameters:\n",
    "        - soup (BeautifulSoup): The BeautifulSoup object representing the page content.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing lists of titles, authors, years, published, abstracts, and abstract URLs.\n",
    "        \"\"\"\n",
    "        all_titles = soup.findAll(\"h3\", attrs={\"class\": \"gs_rt\"})\n",
    "        all_authors = soup.findAll(\"div\", attrs={\"class\": \"gs_a\"})\n",
    "        all_abstracts = soup.findAll(\"div\", attrs={\"class\": \"gs_rs\"})\n",
    "\n",
    "        authors, year, published = [], [], []\n",
    "\n",
    "        titles = [self.get_title(title) for title in all_titles]\n",
    "        abs_url = [self.get_abstract_url(title) for title in all_titles]\n",
    "        abstract = [self.get_abstract(abstr) for abstr in all_abstracts]\n",
    "\n",
    "        for author in all_authors:\n",
    "            auth, yr, publs = self.get_article_info(author)\n",
    "            authors.append(auth)\n",
    "            year.append(yr)\n",
    "            published.append(publs)\n",
    "\n",
    "        return titles, authors, year, published, abstract, abs_url\n",
    "\n",
    "    def get_abstract(self, abstr):\n",
    "        \"\"\"\n",
    "        Extract the text of an article abstract from the provided BeautifulSoup object.\n",
    "\n",
    "        Parameters:\n",
    "        - abstr (BeautifulSoup): The BeautifulSoup object representing the abstract of an article.\n",
    "\n",
    "        Returns:\n",
    "        - str: The text of the article abstract.\n",
    "        \"\"\"\n",
    "        return str(abstr.text).replace(\"\\n\", \"\")\n",
    "\n",
    "    def fetch_web_data(self, records):\n",
    "        \"\"\"\n",
    "        Fetch web data for a specified number of articles and convert it into a DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - records (int): The number of articles to fetch.\n",
    "\n",
    "        Returns:\n",
    "        - pandas.DataFrame: A DataFrame containing information about the fetched articles.\n",
    "        \"\"\"\n",
    "        year_st, year_end = 2012, 2022\n",
    "        columns_google = [\n",
    "            \"Title\",\n",
    "            \"Author\",\n",
    "            \"Year\",\n",
    "            \"Published\",\n",
    "            \"Abstract\",\n",
    "            \"Abstract_UrL\",\n",
    "        ]\n",
    "        titles, authors, years, published, abstract, abs_url = [], [], [], [], [], []\n",
    "        final_data = []\n",
    "\n",
    "        for i in range(0, records, 10):\n",
    "            url = f\"https://scholar.google.com/scholar?start={i}&q=information+retrieval&hl=en&as_sdt=0,44&as_ylo={year_st}&as_yhi={year_end}&as_vis=1\"\n",
    "            soup = self.get_soup(url)\n",
    "            if soup is None:\n",
    "                print(f\"Data Not Fetched for {i} article page\")\n",
    "                continue\n",
    "\n",
    "            a, b, c, d, e, f = self.get_tags(soup)\n",
    "            titles.extend(a)\n",
    "            authors.extend(b)\n",
    "            years.extend(c)\n",
    "            published.extend(d)\n",
    "            abstract.extend(e)\n",
    "            abs_url.extend(f)\n",
    "            print(f\"Fetched {i + 10} articles\")\n",
    "            time.sleep(5)\n",
    "\n",
    "        for i in range(records):\n",
    "            final_data.append(\n",
    "                [titles[i], authors[i], years[i], published[i], abstract[i], abs_url[i]]\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(final_data, columns=columns_google)\n",
    "        print(f\"Number of records: {df.shape[0]}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Example usage with Scholarly class\n",
    "scholarly = Scholarly()\n",
    "df = scholarly.fetch_web_data(int(input(\"Enter num of articles: \")))\n",
    "print(f\"Dimensions of articles: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJd7FoupFDq3"
   },
   "source": [
    "Here, I have fetched the data using with the help of beautiful soup. First I have used the requests module to get the data from scholarly and then put the content inside the beautiful soup to format the html content. Followed by fetched the required attributes using functionalities find, findAll etc. Fetched the articles in past 10 years from 2012 to 2022. Then converted the list of data to dataframe using pandas. I have a total of 6 different attributes: <br>\n",
    "Title: Title of the paper <br>\n",
    "Author: author for the paper or article <br>\n",
    "Year: year in which it published <br>\n",
    "Published : published in which organization <br>\n",
    "Abstract: Basic abstract of the project <br>\n",
    "Abstract_UrL: Url consisting of the abstract data in detailed description <br>\n",
    "\n",
    "Some of the questions, I would like to notice to answer are: <br>\n",
    "\n",
    "Which organization or author has published most number of articles ? <br>\n",
    "What kind of articles or topic related are mostly published in the paper? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l40l4chWYWu_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
